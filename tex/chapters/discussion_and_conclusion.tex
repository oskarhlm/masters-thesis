\chapter{Discussion and Conclusion}\label{cha:discussion-and-conclusion}

Sections \ref{sec:code-interpreter-discussion} and \ref{sec:api-access-discussion} will discuss the test results from \autoref{sec:experimental-results} and provide suggestions as to how the limitations highlighted by the tests can be mitigated. \Autosectionref{sec:conclusion-and-future-work} will conclude this specialization project report, and provide directives for future work on the subject of \acrshort{acr:llm}-powered \acrshort{acr:gis}.

\section{Using ChatGPT's Code Interpreter for Geospatial Analysis}\label{sec:code-interpreter-discussion}

When using ChatGPT's built-in Code Interpreter with file uploads, it became apparent that it runs in a Linux environment and utilizes a mounted drive in the \texttt{/mnt} directory, typically used for temporarily mounted filesystems. In an initial test on the \acrshort{acr:sosi} data format, it tried to execute this \acrshort{acr:gdal} command

$$
    \texttt{ogr2ogr -f "GeoJSON" \{converted\_geojson\_path\} \{sosi\_file\_path\}}
$$

\noindent to perform a conversion from \acrshort{acr:sosi} to GeoJSON, the latter of which is far easier to manipulate in a Python environment. This test failed, and the system's response was that \enquote{the \texttt{ogr2ogr} tool is not available in this environment}.

This result was not very surprising, especially since the driver needed to read and write \acrshort{acr:sosi} files---which is called \textit{fyba}\footnote{\url{https://github.com/kartverket/fyba}} and is developed by the Norwegian Mapping Authority---is almost certainly not available in the standard Linux environment used for ChatGPT's Code Interpreter. As the \acrshort{acr:sosi} standard is still widely used for Norwegian geospatial purposes (though according to its Wikipedia page\footnote{\url{https://no.wikipedia.org/wiki/SOSI-formatet}} expected to be exchanged with the \acrshort{acr:gml} format in the future), it is important for an \acrshort{acr:llm}-based \acrshort{acr:gis} agent focused on the Norwegian market to be able to handle this file type.

The inability of flexibly manipulating the Linux environment used by the Code Interpreter then clearly poses some limitations when developing \acrshort{acr:llm}-based systems. A possible solution is to create a custom environment on a server that we control ourselves. Having the \acrshort{acr:ai} agent run in an environment that we have full control over, gives us greater flexibility, and we can then grant the agent access to powerful \acrshort{acr:gis} tooling, such as the \acrshort{acr:gdal} library. The open-source \enquote{Open Interpreter} project \citep{killianlucasKillianLucasOpeninterpreter2023} could prove useful as an alternative to the closed-source Code Interpreter from OpenAI. Open Interpreter lets us run the interpreter on the computer/server of our choosing, so that it can access the file system directly, as well as libraries and programs stored on the computer/server. It also allows for other \acrshortpl{acr:llm} than \acrshort{acr:gpt}-4, such as Mistral \acrshortpl{acr:llm} and Code-LLaMA. Additionally, with it being an open-source project, one can \enquote{fork} the repository and make project-specific modifications to the interpreter.

\section[Mitigating ChatGPT's Inability to Access Web APIs]{Mitigating ChatGPT's Inability to Access Web \acrshort{acr:api}s}\label{sec:api-access-discussion}

As the results from Test 2 (see \autoref{subsec:experiment-2-results}) show, ChatGPT-4 struggles when provided with URLs to external web \acrshortpl{acr:api}, even when prompted to use its web browsing capabilities and pairing them with its Code Interpreter. These issues are not present when using direct file upload, in which case the model appears to save the uploaded file in a temporary file directory in its Linux environment. Interpreting the inner workings of the Code Interpreter from the code samples in the chat can be challenging, but it appears that it does not do this by default after fetching data from an external web \acrshort{acr:api}. As \autoref{lst:python-for-failed-drammen-outline} shows, it \textit{truncates} the file contents and \enquote{stores} them directly in the code, in an attempt to keep the entire file contents within the context window of the \acrshort{acr:llm}. The context window of the ChatGPT-4 model is currently at 32,000 tokens, and the new GPT-4 Turbo has a context length of 128,000 tokens. While these are of significant size, they are not meant to (or able to) store large file. The size of the context window therefore becomes a limiting factor when the file contents grow large, which is not uncommon for geospatial files.

This is a significant limitation of using ChatGPT-4 out of the box, and one should therefore look into other ways of handling web requests and subsequent storing of the received data. Techniques within \gls{acr:rag}, and libraries like LangChain and Open Interpreter, could help solve this issue.

\glsresetall

\section{Conclusion and Future Work}\label{sec:conclusion-and-future-work}

This specialization project report has presented a literature study in the fields of \acrlong{acr:nlp}, \acrlongpl{acr:llm}, \acrshort{acr:gis}, planning for \acrshortpl{acr:llm}, and \acrlong{acr:rag}, along with three tests that try to demonstrate strengths and weaknesses of \acrshortpl{acr:llm} when dealing with geospatial data. The literature study and tests serve to provide a better starting point when attempting to develop \acrshort{acr:llm}-based \acrshort{acr:gis} agents.

One important finding of the literature study is that there has been a substantial body of work done concerning the potential of using \acrlongpl{acr:llm} in \acrshort{acr:gis} analysis (see \autoref{sec:gis-with-llms}). Studies have been conducted to show that the \acrshort{acr:gpt}-4 model has good geospatial awareness, and there have been created a number of prototypes of autonomous \acrshort{acr:gis} systems powered by \acrshortpl{acr:llm}. \Autosectionref{sec:planning-strategies} discussed various planning strategies that have been developed to facilitate better decision-making and reasoning in \acrshort{acr:llm}-based systems. \Autosectionref{sec:retrieval-automented-generation} introduced \gls{acr:rag}, the concept of providing \acrshortpl{acr:llm} with access to external tooling to help them produce informed and up-to-date responses.

The three tests showcased \acrshort{acr:gpt}-4's abilities of handling geospatial data using various data formats and access channels. The results from Test 1 and 2 showed that it has \textit{some} understanding of how to perform geospatial analysis, but that it also has substantial limitations, e.g., reading and writing large files and accessing data through web \acrshortpl{acr:api}. Test 3 (see \autoref{subsec:ex-3-setup}) served as an initial test of utilizing tools such as LangChain to extend the capabilities of \acrshortpl{acr:llm} like \acrshort{acr:gpt}-4 through programmatic methods.

With this being a specialization project that will transition into a larger master thesis, some points of discussion have been reserved for future work. Additionally, the task of developing a proof of concept has been assigned to the master thesis due to time constraints and the intention to acquire more knowledge before proceeding with development. Subsections \ref{subsec:memory-and-embeddings} and \ref{subsec:testing-regime} will elaborate upon potentially important issues that should be addressed when developing \acrshort{acr:llm}-based \acrshort{acr:gis} agents.

% \subsection{Balancing Accuracy Against Performance and Costs}

% The ecosystem of agent frameworks and planning strategies to improve agent performance on complex tasks (discussed in \nameref{cha:related-work}), is a growing one. Different agents frameworks and planning strategies should be compared to see which are most viable for \acrshort{acr:gis} work. Important considerations are the ability to destructure complex problems, the ability to take advantage of external tooling, and computational time. More complex planning strategies typically demand more interactions with \acrshort{acr:llm} \acrshortpl{acr:api}, which can be expensive both in terms of computational time and cost (when using a monetized \acrshort{acr:api} like OpenAI's for \acrshort{acr:gpt}-4 and \acrshort{acr:gpt}-4).

% \cite{clearyLatencyBenchmarksComparisons2023} did benchmarking of different \acrshortpl{acr:llm} on different providers. Important takeaways were that \acrshort{acr:gpt}-4 is about 6.3 times slower than \acrshort{acr:gpt}-3.5-Instruct, and that Azure has far lower latency in most cases for inference on \acrshort{acr:gpt} models. Such considerations are important when addressing usability of \acrshort{acr:llm}-based applications, balancing accuracy against speed and cost. Using free open-source alternatives where possible is a good option to reduce cost. Open-source software like the Open Interpreter project should also be investigated.

\subsection{Memory and Embeddings}\label{subsec:memory-and-embeddings}

Storing information for future use is important when developing \acrshort{acr:llm}-based agents in order for them to produce consistent responses. \cite{wengLLMPoweredAutonomous2023} presents three different types of memory in human brains: (1) \textit{Sensory Memory}, (2) \textit{Short-Term Memory}, and (3) \textit{Long-Term Memory}. When translated to \acrshortpl{acr:llm}, we can think of \textit{Sensory Memory} as learning embedding representations, \textit{Short-Term Memory} as the memory contained within the limits of the context window of the Transformer, and  \textit{Long-Term Memory} as an external vector store that can be attended to by the agent at query time. Such a vector store/database would store the vector embeddings of the data contained within it, and allow for fast and accurate similarity search and retrieval based on the vector distance or similarity between the vector representations \citep{evchakiVectorDatabase2023}. \cite{wengLLMPoweredAutonomous2023} lists some common approximate nearest neighbours algorithms for fast retrieval speeds, including \gls{acr:lsh} and \gls{acr:faiss}.

Future work should build upon the research of \cite{unluChatmapLargeLanguage2023} (see \autoref{sec:gis-with-llms}) and investigate whether vector embeddings can be utilized for long-term storage of geospatial data with textual descriptions, or if a vector database can efficiently retrieve relevant resources such as \acrshortpl{acr:api} or other external tools based on their documentation/specifications. Furthermore, these documentations and \acrshort{acr:api} specifications can be large is size, and the context length could become a limiting issue. Vector embeddings can help mitigate such issues. By splitting the documents into chunks and indexing them using vector embeddings, one can extract only the relevant parts and pass these to the \acrshort{acr:llm} with the prompt.

\subsection{Testing Regime}\label{subsec:testing-regime}

In order to test the feasibility of different language models to serve as the brain of an autonomous \acrshort{acr:gis} agent, a testing regime should be developed. In the examples of autonomous \acrshort{acr:gis} agents described in the literature study of this report (see \autoref{sec:gis-with-llms}), results were generally presented in the form of case studies. This type of qualitative testing is entirely appropriate for showcasing the possibilities of the technologies, but it may be insufficient for comparing the performance of \textit{different} systems. A quantitative approach would probably be preferable.

One idea is to create a test dataset which consists of inputs and corresponding desired outputs of typical \acrshort{acr:gis} tasks. Inputs would in this case be natural language queries inputted by a mock user, and the outputs would be what you would expect a \acrshort{acr:gis} professional to return when given the same tasks/queries. Inputs should reflect the varying level of \acrshort{acr:gis} knowledge between different user groups (see \autoref{sec:user-groups}). Outputs could be files with typical geospatial extensions (.shp, .geojson, .sos, etc.), or they could adhere to API specifications from geospatial standards (see \autoref{sec:geospatial-standards}).

While the inputs should be fairly simple to construct, there are several questions to be answered in regard to the outputs, among which are the following:

\begin{itemize}
    \item How does one evaluate the accuracy of the output?
    \item How should the \acrshort{acr:ai} agent respond when the user does not specify an output file format?
    \item How does one evaluate the usefulness of responses to questions that should \textit{not} return geospatial files, e.g., answers to general questions about geo-related subjects?
\end{itemize}

\noindent These questions are outside the scope of this specialization project, and are therefore left out for future work.



\glsaddall