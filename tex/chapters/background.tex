\chapter{Background Theory and Related Work}
%OR: \chapter{Tools and Methods}
\label{cha:background-theory}

\begin{comment}
The background theory depth and breadth depend on the depth needed to understand your project
in the different disciplines that your project crosses.
It is not a place to just write about everything you know that is vaguely connected to your project.
The theory is here to help the readers that do not know the theoretical basis of your work so that they
can gain sufficient understanding to understand your contributions --- and also for yourself to show that
you have understood the underlying theory and are aware of the methods used in the field.
In particular, the theory section provides
an opportunity to introduce terminology that can later be used without disturbing the text with a definition.
In some cases it will be more appropriate to have a separate section for different theories (or even separate chapters).
However, be careful so that you do not end up with too short sections.
Subsections may also be used to separate different background theories.

Be aware that ``background'' is a general term that refers to everything done by somebody else,
in contrast to the ``foreground'', which is your own work.
Hence there can (and will) be several background chapters, with the background theory being one of them
--- or several of them, since it thus is quite possible to split the background theory over more than one chapter,
e.g., by having a chapter introducing the theory directly needed for the research field in question and another
chapter discussing the machine learning theory, algorithms, tools, and evaluation methods commonly used in the field.
The related work chapter is thus also part of the background, while a chapter about data might be background
(if you only use somebody else datasets), but can also be part of the foreground (if you collect and/or annotate data
yourself, or if you process or clean the data in ways that can make it part of your own contribution).

It is ok to reuse material from other texts that you have written (e.g., the specialisation project), but if you do so, that must be clearly stated in the text, together with a description of how much of the text is new, old or rewritten/edited.
Such a statement about recycling of material in the Background Theory chapter can thus come here in the chapter introduction.

\section{Writing References in the Text}
\label{sec:writing_references}

When introducing techniques or results, always reference the source.
Be careful to reference the original contributor of a technique and not just someone who happens to use the technique.%
\footnote{But always make sure that you have read the work you are citing --- if not, cite someone who has!}
For results relevant to your work,
you would want to look particularly at newer results so that you have referenced the most up-to-date work in your area.
A common rule of thumb is to at least reference the first paper introducing the issue and the paper containing the latest / state-of-the-art
results. Additional papers making substantial contributions should also be referenced, as well as of course the ones you find most interesting.
Remember to use the right verb form depending on the number of authors.

If you do not have the source handy when writing, mark in the text that a reference is needed and add it later. \todo{add reference}
Web pages are not reliable sources --- they might be there one day and removed the next; and thus should be avoided, if possible.
A verbal discussion is not a source and should normally not be referenced
(though you can reference ``personal communication'', if there are no other options).
The bulk of citations in the report will appear in Chapter~\ref{cha:related_work}.
However, you will often need to introduce some terminology and key citations already in this chapter.

You can cite a paper in the following manner (and several other versions,
see the \verb!natbib! package documentation):

\begin{enumerate}[(i)]
    \item When referring to authors, using their names in the text:\\
          \citet{Authorson;Bobsen:10} stated something rather nice.
          (using \verb!\citet!)
    \item To cite indirectly: \\
          Papers should be written nicely \citep{Authorson;Bobsen:10}
          (using \verb!\citep!)
          {\em or\/}\\
          In \citet{Authorson;Bobsen:10}, a less detailed template was presented.
    \item To just cite the authors: \\
          \citeauthor{Authorson;Bobsen:10} wrote a nice paper
          (using \verb!\citeauthor!).
    \item Or just the year: \\ \citeyear{Authorson;Bobsen:10}
          (using \verb!\citeyear!).
    \item You can even cite specific pages or chapters: \citet[p. 3]{Authorson;Bobsen:10}
          (using \verb!\citet[...]{...}!).
\end{enumerate}

You should obviously always cite your supervisor's work \citep{BenyonEA:13},
even if it is completely irrelevant \citep{Das;Gamback:13a} or very old \citep{AlshawiEA:91b}.
Digging up an even older book can also appear impressive \citep{Diderichsen:57}.
(Or? ;-)

\section{The Reference List}
\label{sec:reference_list}

In general, make sure that the references that appear in your reference list can be easily located and identified by the reader.
So include not only authors and title, but year and place of publication, the full names of conferences and workshops,
page numbers in proceedings and collections, etc.
Hyperlinks or \acrfull{acr:doi} numbers are also nice to include.
Just as in the text itself, it is important to be consistent in the reference list, so include the same type of information for all references and write it in the same way.

Check out the reference list at the end of this document for examples of how to write references in \BibTeX.
Note a particular quirk: Many \BibTeX\ styles convert uppercase letters to lowercase, unless specifically told not to.
You might thus need to ``protect'' characters that should not be converted, e.g., by writing \texttt{\{T\}witter} as in the \citet{FountaEA:18} reference.

Also, keep in mind that `et' is a word in its own right (`and'), so there is no period after it (even though there is a period after `al.', which is short for `alia', meaning `others').
Of course, when including such a reference in the text, the authors should be referred to in plural form.
So \citet{BenyonEA:13} state that life is good (not ``states'').

Many sites, such as journals and \url{dblp.org} provide the matching \BibTeX\ entry for a reference.
However, you might still need to edit the entry in order to be consistent with the rest of your references.
If you find references from sites such as \url{scholar.google.com} or \url{arXiv.org}, keep in mind that they often not are complete,
so that you might need to add information to the entry (and probably edit it as well).

Some other good sites to find state-of-the-art work:
\begin{itemize}
    \item \url{paperswithcode.com}
    \item \url{nlpprogress.com}

\end{itemize}


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\columnwidth]{figs/figure1.pdf}
    \caption[Boxes and arrows are nice]{Boxes and arrows are nice (adapted from \citealp{Authorson;Bobsen:10}, reprinted with permission)}
    \label{fig:BoxesAndArrowsAreNice}
\end{figure}

\section{Introducing Figures}

\LaTeX is a bit tricky when it comes to the placement of ``flooting bodies'' such as figures and tables. It is often a good idea to let their code appear right before the header of the (sub)section in which they appear.
Note that you should anyhow always use an option for the placement (e.g., \verb|[t!]| to place it at the top of a page).

Remember that if you reproduce someone else's figures you must credit the original author --- such as
Figure~\ref{fig:BoxesAndArrowsAreNice} (adapted from \citealp{Authorson;Bobsen:10}),
as well as state that you have permission to reprint it (e.g., if it is published under a Creative Commons License,
or if you have gained explicit permission from the author).

Do not just put the figure in and leave it to the reader to try to understand what the figure is.
The figure should be included to convey a message and you need to help the reader to understand the message
intended by explaining the figure in the text.
Hence \textbf{all} figures and tables should always be referenced in the text, using the \verb!\ref! command.
It is good practice to always combine it with a non-breakable space (\verb!~!) so that there will be no newline between the term referring to it and the reference, that is, using \verb!Figure~\ref{fig:BoxesAndArrowsAreNice}!.

If a figure appears far from the text explaining it,
it is a good idea to add its page number (using the \verb!\pageref! command), so that you can refer to Figure~\ref{fig:BoxesAndArrowsAreNice} (on Page~\pageref{fig:BoxesAndArrowsAreNice}).

Also, note that you can have a longer version of the figure (and table) caption attached to the actual figure,
while using the optional first argument to \verb!\caption! to include a shorter version in the list of figures (lof) or list of tables:
\begin{quote}
    \begin{verbatim}
\caption[Shorter lof text]{Longer text appearing under the figure}
\end{verbatim}
\end{quote}

It is good practice to add a note about a missing figure in the text,
such as the completely amazing stuff that will appear in Figure~\ref{fig:AmazingFigure}.

\begin{figure}[t!]
    \centering
    \missingfigure{Here we will add an amazing figure explaining it all}
    \caption{A missing figure}
    \label{fig:AmazingFigure}
\end{figure}

In general it is good to add notes about things that you plan on writing later.
The \verb!todonotes! package is great for that kind of book-keeping, letting you write both shorter comments in the margin\todo{l8r dude} and longer comments inside the text, using the option \verb![inline]!.
\todo[inline]{There are always some more stuff that you will need to add at some later point.
    Be sure to at least make a note about it somewhere.}

\section{Introducing Tables in the Report}

\newcommand\emc{-~~~~}
\begin{table}[t!]
    \centering
    \caption[Example table]{Example table (F$_1$-scores); this table uses the optional shorter caption that will appear in the list of tables, so this long explanatory text will not appear in the list of tables and is only here in order to explain that to the reader.}
    \begin{tabular}{c|c|rrrrrr}
        \tabletop
        Langs                  & Source                                           & \multicolumn{1}{c}{Lang1} & \multicolumn{1}{c}{Lang2} & \multicolumn{1}{c}{Univ} & \multicolumn{1}{c}{NE} & \multicolumn{1}{c}{Mixed} & \multicolumn{1}{c}{Undef}
        \\ \tablemid
        \multirow{5}{*}{EN-HI} & FB+TW                                            & 54.22                     & 22.00                     & 19.70                    & 4.00                   & 0.05                      & 0.03                      \\
                               & FB                                               & 75.61                     & 4.17                      & 18.00                    & 2.19                   & 0.02                      & 0.01                      \\
                               & TW                                               & 22.24                     & 48.48                     & 22.42                    & 6.71                   & 0.08                      & 0.07                      \\
                               & Vyas                                             & 54.67                     & 45.27                     & 0.06                     & \emc                   & \emc                      & \emc                      \\
                               & FIRE                                             & 45.57                     & 39.87                     & 14.52                    & \emc                   & 0.04                      & \emc                      \\ \tablemid
        \multirow{2}{*}{EN-BN} & TW                                               & 55.00                     & 23.60                     & 19.04                    & 2.36                   & \emc                      & \emc                      \\
                               & FIRE                                             & 32.47                     & 67.53                     & \emc                     & \emc                   & \emc                      & \emc                      \\ \tablemid
        EN-GU                  & FIRE                                             & 5.01                      & \textbf{94.99}            & \emc                     & \emc                   & \emc                      & \emc                      \\
        \tablemid
        DU-TR                  & Nguyen                                           & 41.50                     & 36.98                     & 21.52                    & \emc                   & \emc                      & \emc                      \\ \tablemid

        EN-ES                  & \multirow{4}{*}{\rotatebox[origin=c]{90}{EMNLP}}
                               & 54.79                                            & 23.50                     & 19.35                     & 2.08                     & 0.04                   & 0.24                                                  \\
        EN-ZH                  &                                                  & 69.50                     & 13.95                     & 5.88                     & 10.60                  & 0.07                      & \emc                      \\
        EN-NE                  &                                                  & 31.14                     & 41.56                     & 24.41                    & 2.73                   & 0.08                      & 0.08                      \\
        AR-AR                  &                                                  & 66.32                     & 13.65                     & 7.29                     & 11.83                  & 0.01                      & 0.90                      \\ \tablebot
    \end{tabular}
    \label{tab:ExampleTable}
\end{table}

As you can see from Table~\ref{tab:ExampleTable}, tables are nice.
However, again, you need to discuss the contents of the table in the text.
You do not need to describe every entry, but draw the reader's attention to what is important in the table,
e.g., that 94.99 is an amazing F$_1$-score (and that probably something fishy happened there).
Use boldface, boxes, colours, arrows, etc. to mark the important parts of the table.

As can be seen in the example, elements in a table can sometimes benefit from being rotated (such as EMNLP in the `Source' column) or cover more than one row (EMNLP, as well as EN-HI and EN-BN in the `Langs' column) --- or more than one column, for that matter.

\end{comment}

\begin{itshape}
    NB! Parts of this chapter is reused material from the specialization project \citep{holmLLMsDeathGIS2023} that preceded this master's thesis. The following sections have been reused, with some adjustments: \autoref{subsec:attention-and-the-transformer-architecture}, \autoref{subsec:sota-decoder-only-llms} (\nameref{subusubsec:gpt}), and \autoref{subsec:llm-gis}.
\end{itshape}

\vspace{12pt}

\noindent \Autochapterref{cha:background-theory} will lay a theoretical basis for the work done in this master thesis, providing the reader with the necessary knowledge to understand the contributions of the work. \Autosectionref{sec:llms} will focus on \glspl{acr:llm}. First, \autoref{subsec:core-concepts} will introduce the terms \enquote{token} and \enquote{context window}, which are used frequently throughout this thesis. Thereafter, \autoref{subsec:attention-and-the-transformer-architecture} will present the component that most modern \glspl{acr:llm} are based upon --- namely the Transformer --- and the attention mechanism that powers it. Continuing, \autoref{subsec:sota-decoder-only-llms} will present some of the leading \acrshortpl{acr:llm} as of \today, both proprietary and open-source ones. \Autosubsectionref{subsec:prompt-engineering} will then present the concept of \textit{prompt engineering}, which is a structured way of composing input to an \acrshort{acr:llm}, before \autoref{subsec:function-calling} concludes \autoref{sec:llms} by introducing \textit{function calling}, a way of allowing \acrshortpl{acr:llm} to use external tools. \Autosectionref{sec:langchain} will give a brief intro to LangChain, a Python library that is used extensively throughout GeoGPT's code base. \Autosectionref{sec:geo-dbs-and-data-catalogues} will introduce the reader to PostGIS and \acrshort{acr:ogc} \acrshort{acr:api} Features, which are geospatial technologies that are used within GeoGPT. \Autosectionref{sec:related-work} will present work that in some way relates to this thesis. \Autosubsectionref{subsec:llm-gis} presents work where \acrshortpl{acr:llm} are applied for geospatial purposes, while \autoref{subsec:agent-patterns} describes different patterns that can be applied to \acrshort{acr:llm}-based agents to improve their performance.


\section[Large Language Models]{\acrlongpl{acr:llm}}
\label{sec:llms}

This section will give the reader an overview of the inner workings of \acrlongpl{acr:llm}. \glspl{acr:llm} are a type of neural networks that excel at language processing. They can be developed for different \gls{acr:nlp} tasks, for instance text classification, masked language modelling, and text generation. While they all have their use cases, only text generation will be relevant for this thesis. Generative \glspl{acr:llm} are designed to understand and generate sequences of text, and are the types of models behind technologies like OpenAI's \textit{ChatGPT} \citep{openaiIntroducingChatGPT2022}.

\subsection{Tokens and Context Window}
\label{subsec:core-concepts}

While us humans understand sentences as sequences of words, \acrshortpl{acr:llm} perceive them as sequences of \textbf{tokens}. An \acrshort{acr:llm} possesses a fixed set of unique tokens in its vocabulary, from which it constructs words and sentences. Tokens can be entire words, short character sequences (subwords), or single characters \citep{aliTokenizerChoiceLLM2024}. \autoref{fig:tokenization-example-sentence} illustrates how OpenAI's \acrshort{acr:gpt}-3.5 and \acrshort{acr:gpt}-4 models \textit{tokenize} an English sentence. Notice how some words are deconstructed into more than one token. For example, the word \enquote{revolutionizing} is split into its stem, \enquote{revolution}, and its suffix, \enquote{izing}. \autoref{fig:tokenization-revolution} shows how this process applies to other suffixes as well. This way, the model only has to learn the meaning of \enquote{revolution}, and can append different suffixes to modify its function within a sentence, as opposed to learning the meaning of an entirely new token for each version of the word.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{tokens_illustration.png}
    \caption{Tokenization of an English sentence using the \acrshort{acr:gpt}-3.5 Turbo and \acrshort{acr:gpt}-4 tokenizer}
    \label{fig:tokenization-example-sentence}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.25\textwidth]{revolution_tokens.png}
    \caption{Tokenization of the word \enquote{revolution}, with different suffixes}
    \label{fig:tokenization-revolution}
\end{figure}

The \textbf{context window} of an \acrshort{acr:llm} is the range of tokens that an \acrshort{acr:llm} is able to process \citep[1]{zhuPoSEEfficientContext2024}. When an \acrshort{acr:llm} generates text, it does so by generating a new token based on the tokens it sees in the span of the context window. A larger context window will allow the \acrshort{acr:llm} to take more and longer documents as context, and generate output based on these. Leveraging sophisticated techniques like \gls{acr:rope} \citep{suRoFormerEnhancedTransformer2024} and \gls{acr:pose} training \citep{zhuPoSEEfficientContext2024}, researchers have been able to efficiently extend the context window of open-source models like, for instance, Meta AI's \acrshort{acr:llama} models (introduced in \autoref{subsec:sota-decoder-only-llms}).


\subsection{Attention and the Transformer Architecture}
\label{subsec:attention-and-the-transformer-architecture}

Titling their paper \enquote{\citetitle{vaswaniAttentionAllYou2017}}, \cite{vaswaniAttentionAllYou2017} suggest that their Transformer architecture renders network architectures like \glspl{acr:rnn} redundant for machine translation purposes, due to the Transformers superior parallelization abilities and the shorter path between combinations of position input and output sequences, making it easier for the model to learn long-range dependencies \citep[6]{vaswaniAttentionAllYou2017}. The Transformer is able to process multiple positions in the input sequence simultaneously, unlike \acrshortpl{acr:rnn}, which are purely sequential. Furthermore, the Transformer employs \textit{self-attention}, which enables the model to draw connections between arbitrary parts of a given sequence, bypassing the long-range dependency issue commonly found with \glspl{acr:rnn}. Basically, this makes it easier for the model to \enquote{remember} parts from earlier in the input sequence when generating new tokens.

An attention function maps a \textit{query} and a set of \textit{key-value pairs} to an output, calculating the compatibility between a query and a corresponding key \citep[3]{vaswaniAttentionAllYou2017}. Looking at \citeauthor{vaswaniAttentionAllYou2017}'s proposed attention function \eqref{eq:attention}, we observe that it takes the dot product between the query $Q$ and the keys $K$, where $Q$ is the token that we want to compare all the keys to. Keys similar to $Q$ will get a higher score, i.e., be \textit{more attended to}. Finally, everything is passed into the softmax function, which normalizes the distribution of probabilities. The final matrix multiplication with the values $V$, the initial embeddings of the input tokens, will yield a new embedding in which all individual tokens have some context from all other tokens. The attention mechanism is improved by multiplying queries, keys, and values with \textit{learned} weight matrices that are obtained through backpropagation. Self-attention is a special kind of attention in which queries, keys, and values are all the same sequence.

\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
    \label{eq:attention}
\end{equation}

% \begin{equation}
%     \text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}.
%     \label{eq:softmax}
% \end{equation}

Attention blocks can be found in three places in the Transformer architecture \citep[5]{vaswaniAttentionAllYou2017}, which is displayed in \autoref{fig:transformer-architecture}. The Transformer, which was created for machine translation tasks, has two main parts: the \textit{encoder} (the left part of \autoref{fig:transformer-architecture}) and the \textit{decoder} (the right part of \autoref{fig:transformer-architecture}). The encoder learns the representation of the origin language from a given input sequence, while the decoder learns the representation in the target language and generates the output sequence. Below is an example of Norwegian-to-German translation, showcasing the different ways that the attention mechanism is used:

\begin{enumerate}
    \item In the encoder block, to perform self-attention on the Norwegian input sequence.
    \item In the decoder block, to perform self-attention on the German output sequence.
    \item In the decoder block, to perform \textit{cross-attention} (also known as encoder-decoder attention), where each position in the decoder attends to all positions in the encoder.
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{transformer_architecture.png}
    \caption[The encoder-decoder architecture of the Transformer]{The encoder-decoder architecture of the Transformer \citep[3]{vaswaniAttentionAllYou2017}}
    \label{fig:transformer-architecture}
\end{figure}

The Transformer achieved new state-of-the-art results for machine translation tasks. It represented a breakthrough in the field of \gls{acr:nlp}, and is the fundamental building block of most modern \glspl{acr:llm}. Though designed for machine translation, the architecture has later proved effective for numerous \acrshort{acr:nlp} tasks, and for a variety of modalities.


\subsection[State-of-the-Art Decoder-Only LLMs]{State-of-the-Art Decoder-Only \acrlongpl{acr:llm}}
\label{subsec:sota-decoder-only-llms}

While the work of \cite{vaswaniAttentionAllYou2017} is still considered perhaps the greatest breakthrough in \gls{acr:nlp}, most modern \acrshortpl{acr:llm} do not apply the exact encoder-decoder architecture of the Transformer. The evolution following the Transformer has favoured generative decoder-only models, focusing entirely on the generative component of the Transformer, with the goal being to create models that can produce coherent and context-aware text.

\subsubsection{The GPT Family}
\label{subusubsec:gpt}

\gls{acr:gpt} is an \acrshort{acr:llm} that was introduced by OpenAI in 2018 \citep{radfordImprovingLanguageUnderstanding2018}. Designed specifically for text generation, the \acrshort{acr:gpt} is essentially a stack of Transformer \textit{decoders}. It demonstrates through its vast pre-training on unlabelled data that such \textit{unsupervised} training can help a language model learn good representations, providing a significant performance boost while alleviating the dependence on \textit{supervised} learning, which requires labelled training data. The model employs masked \textit{multi-head attention}, which runs the input sequence through multiple attention heads in parallel. It is restricted to seeing only the last $k$ tokens, with $k$ being the size of the context window, and is tasked with predicting the next one.

Training consists of two stages: the above-mentioned unsupervised pre-training process, and supervised fine-tuning. The former is used to find a good initialization point, essentially teaching the model to imitate the corpora that it is trained on. This, however, will produce undefined behaviour where the model rambles on uncontrollably, simply attempting to elaborate upon the input sequence it is given to the best of its abilities. It is therefore necessary to fine-tune the model on target tasks in a \textit{supervised} manner. \cite[4]{radfordImprovingLanguageUnderstanding2018} explain how the model can be fine-tuned directly on tasks like text classification, but how other tasks require the conversion of structured inputs into ordered sequences, as is the case when fine-tuning for tasks like multiple choice. \autoref{fig:multiple-choice} shows this \textit{inlining} process, which is required because the pre-trained model was trained on contiguous sequences of text. In the case of ChatGPT, \citeauthor{openaiIntroducingChatGPT2022} used \gls{acr:rlhf} \citep{christianoDeepReinforcementLearning2023} by employing a three-step strategy: first training using a supervised policy, then using trained reward models to rank alternative completions produced by ChatGPT models, before fine-tuning the model using \gls{acr:ppo} \citep{schulmanProximalPolicyOptimization2017}, which is a way of training \acrshort{acr:ai} policies. This pipeline is then performed for several iterations until the model produces the desired behaviour \citep{openaiIntroducingChatGPT2022}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{multiple-choice.png}
    \caption[Inlining of a multiple choice task for GPT fine-tuning]{An illustration showing how multiple choice tasks are \textit{inlined} so that they can be used for fine-tuning. This is because the \acrshort{acr:gpt} is trained on ordered sequences \citep[4]{radfordImprovingLanguageUnderstanding2018}.}
    \label{fig:multiple-choice}
\end{figure}

OpenAI's \acrshort{acr:api} currently features three flagship models, which are all proprietary: the \acrshort{acr:gpt}-3.5 Turbo model, which is fast and inexpensive; the \acrshort{acr:gpt}-4 Turbo model, which as of \today~is described by OpenAI as their \enquote{previous high-intelligence model}; and lastly, \acrshort{acr:gpt}-4o, their \enquote{fastest and most affordable flagship model}.\footnote{\url{https://openai.com/api/} (last visited on 2nd June 2024)} The \acrshort{acr:gpt}-4o has unique multi-modal abilities and can reason across audio, vision, and text in real time, according to OpenAI.\footnote{\url{https://openai.com/index/hello-gpt-4o/} (last visited on 2nd June 2024)}

\subsubsection{The Gemini Family}
\label{subsubsec:gemini}

The suite of models known as \textit{Gemini} is Google's latest response OpenAI's \acrshort{acr:gpt} models. The Gemini 1.0 suite \citep{geminiteamGeminiFamilyHighly2024}, released in December 2023, is the first suite of Gemini models and includes three different versions: \textit{Ultra}, \textit{Pro}, and \textit{Nano}. These are listed in descending order in terms of size. The models of the Gemini 1.0 suite are multi-modal, supporting text, image, audio, and video. Gemini 1.0 Ultra displayed new state-of-the-art performance on most major benchmarks, but performed significantly worse on the \textit{HellaSwag} benchmark \citep{zellersHellaSwagCanMachine2019} compared to the latest \acrshort{acr:gpt}-4 model at the time. The HellaSwag benchmark measures a model's common-sense understanding, and the models scored 87.8\% and 95.3\%, respectively. Supporting a context length up to 1M tokens, Gemini 1.0 Ultra surpassed Claude 2.1's context window of 200k with a wide margin (see the next paragraph for more on the Claude models), and with the release of Gemini 1.5 Pro in February 2024 came also the possibility of utilizing a context window of up to 10M tokens, though in production this number is currently limited to 1M tokens \citep{geminiteamGeminiUnlockingMultimodal2024, pichaiOurNextgenerationModel2024}. Furthermore, Gemini 1.5 \textit{Pro} outperforms Gemini 1.0 \textit{Ultra} in some capabilities despite using significantly less training compute \citep[31]{geminiteamGeminiUnlockingMultimodal2024}.

\subsubsection{The Claude Family}
\label{subsubsec:claude}

Developed at Anthropic, \textit{Claude} is the third major suite of proprietary \acrshortpl{acr:llm}. Anthropic has helped push in the direction of long-context \acrshortpl{acr:llm}, with their Claude 2 model, released in November 2023, being the first to support up to 200k tokens \citep[9]{anthropicModelCardEvaluations2023}. Latest in line is Claude 3, a family of language models that --- much like the Gemini family --- features three models of different sizes: \textit{Opus}, \textit{Sonnet}, and \textit{Haiku}. Again, these are listed in descending order in terms of size. The Opus model offers the most advanced capabilities, and outperforms \acrshort{acr:gpt}-4 and Gemini 1.0 Ultra on most benchmarks \citep[6]{anthropicClaudeModelFamily2024}. Haiku is Anthropic's fast and economical option, while Sonnet serves as a balance between Opus' complexity and Haiku's speed.

\subsubsection{Open-Source Alternatives}
\label{subsubsec:open-source-llms}

As mentioned, OpenAI's \acrshort{acr:gpt} models, Google's Gemini models, and Anthropic's Claude models are all proprietary. This prevents developers from downloading these models and making improvements and customizations to them. This has led to the emergence of a number of \textit{open-source} \acrshortpl{acr:llm}.

The \textbf{Llama} family of \acrshortpl{acr:llm}, developed at Meta AI, is perhaps the most famous open-source option to the proprietary \acrshortpl{acr:llm}. At the time of writing, their latest \acrshort{acr:llm} is the \textit{\acrshort{acr:llama} 3} model \citep{metaaiIntroducingMetaLlama2024}, which comes in two sizes: 8B and 70B parameters. Both models display state-of-the-art performance on most major benchmarks compared to similar open-source alternatives, and the 70B model even surpasses proprietary models like Gemini 1.5 Pro and Claude 3 Sonnet on certain benchmarks.

\textbf{Mistral AI} is one of the most prominent actors in the world of open-source \acrshortpl{acr:llm}. Their debut model, the \textit{Mistral 7B}, outperformed \acrshort{acr:llama} 2 13B (the best open-source \acrshort{acr:llm} at the time) across all the benchmarks they evaluated \citep{jiangMistral7B2023}. Mistral AI has also gained fame for their \gls{acr:smoe} architecture, which was introduced with the \textit{Mixtral 8x7B} model \citep{jiangMixtralExperts2024}. Mixtral 8x7B shares the same architecture as Mistral 7B, except that each layer of the model is composed of 8 feed-forward blocks. Using a router at each layer, Mixtral 8x7B is able to use only 13B out of a total of 47B parameters during inference, keeping cost and latency low.

Along with their Gemini models, Google released a family of open-source models called \textbf{Gemma}, which are based on the same research conducted for the Gemini models \citep{gemmateamGemmaOpenModels2024}. Gemma comes in two sizes: 2B and 7B parameters. At its release, the Gemma 7B surpassed \acrshort{acr:llama} 2 13B and Mistral 7B in 11 out of 18 benchmarks. Note, however, that \acrshort{acr:llama} 3 8B has improved upon its predecessor, and now also performs better than Gemma 7B, overall.


\subsection{Prompt Engineering}
\label{subsec:prompt-engineering}

Prompt engineering refers to a structured process of constructing input to an \acrshort{acr:llm}. In a chat-based context like that of ChatGPT, the prompt generally consists of a series of messages. These messages can hold one of three different roles,\footnote{\url{https://platform.openai.com/docs/guides/text-generation/chat-completions-api} (last visited on 2nd June 2024)} each listed below:

\begin{itemize}
    \item \textbf{System}: Generally used to set the behaviour of the \acrshort{acr:llm} assistant, giving the assistant a specific personality or information on how to answer the user.
    \item \textbf{User}: A message from the human user that the assistant should respond to.
    \item \textbf{Assistant}: A message generated by the \acrshort{acr:ai}/\acrshort{acr:llm}.
\end{itemize}

\cite{whitePromptPatternCatalog2023} stress the importance of prompt engineering to efficiently converse with \glspl{acr:llm}. They provide a catalogue of prompt patterns that aim to help enforce certain qualities in the output generated by the \gls{acr:llm}. These patterns are organized into six categories \citep[4]{whitePromptPatternCatalog2023}:

\begin{itemize}
    \item \textbf{Input Semantics}: Clarifying what information is fed into the \acrshort{acr:llm} and how this input should be used to generate responses.
    \item \textbf{Output Customization}: Strategies to guide how the \acrshort{acr:llm} should format and structure its responses.
    \item \textbf{Error Identification}: Methods of identifying and resolving errors in the outputs produced by the \acrshort{acr:llm}.
    \item \textbf{Prompt Improvement}: Techniques to improve the quality of both the input provided to the \acrshort{acr:llm} and the output it generates.
    \item \textbf{Interaction}: Strategies for enhancing interactions between the user and the \acrshort{acr:llm}.
    \item \textbf{Context Control}: Controlling the contextual domain within which the \acrshort{acr:llm} operates.
\end{itemize}

Patterns that turned out useful to GeoGPT --- the main contribution of this thesis --- include the \textit{Template} pattern (Output Customization), the \textit{Reflection} pattern (Error Identification), and the \textit{Infinite Generation} pattern (Interaction). The \textit{Template} pattern allows the user or the system to define a template for the \acrshort{acr:llm} to fill out. This is closely related to \textit{function calling}, which is discussed in \autoref{subsec:function-calling}. Also related to function calling is the \textit{Reflection} pattern, which allows the \acrshort{acr:llm} to inspect its own output in order to identify and correct errors. The \textit{Infinite Generation} pattern lets the \acrshort{acr:llm} generate output indefinitely without requiring the user to re-enter the conversation after each generated message. This pattern is important when developing agentic behaviours.


\subsection[Function Calling LLMs]{Function Calling \acrshortpl{acr:llm}}
\label{subsec:function-calling}

\textit{Function calling} --- also known as \textit{tool calling} --- was first introduced by OpenAI in April 2023 \citep{eletiFunctionCallingOther2023}. Function calling allows developers to provide function definitions to an \gls{acr:llm}, and have the \gls{acr:llm} decide, based on the current context, \textit{if} a function should be invoked, and what \textit{parameters} should be used as input to the function. \autoref{code:tool-definition-example} shows a description of a function that performs mathematical division. It is specified by its name, description, and its two parameters: the dividend and the divisor, both of which are required. This function definition has a corresponding function in code written by the developer, which can now be invoked when the \acrshort{acr:llm} deems it necessary. After a function has been invoked using the parameters specified by the \acrshort{acr:llm}, the \acrshort{acr:llm} can be prompted once more, now with the output of the function invocation included in the prompt. This way, function calling makes it possible to give an \gls{acr:llm} \textit{hooks} into the real world, and provides a more reliable way for developers to integrate \glspl{acr:llm} into applications.

\begin{lstlisting}[
    language=json,
    caption={[Example of a function definition for a tool that performs mathematical divison]Example of a function definition for a tool that performs mathematical divison. It is specified by a name and description, and its two parameters: the dividend and the divisor, both of which are required.},
    label=code:tool-definition-example,
    float, floatplacement=H
]
{
  "type": "function",
  "function": {
    "name": "divide",
    "description": "Performs a mathematical division.",
    "parameters": {
      "type": "object",
      "properties": {
        "dividend": {
          "description": "The number to be divided (numerator).",
          "type": "number"
        },
        "divisor": {
          "description": "The number by which the dividend is divided (denominator).",
          "type": "number"
        }
      },
      "required": ["dividend", "divisor"]
    }
  }
}
\end{lstlisting}

Function calling is commonly used to provide the \acrshort{acr:llm} with correct and up-to-date information. Having the \gls{acr:llm} use function calling for information retrieval also makes them more transparent by making it possible to trace a claim back to its source, something that is normally hard to do with \glspl{acr:llm}. Another use case is code execution. Take this rather simple function signature:

$$
    \texttt{execute\_python\_code(code: string) -> string}
$$

Such a function could take a string of Python code and return the standard output that results from executing that code. This is the principle behind what was previously known as ChatGPT's \enquote{Code Interpreter} mode, where ChatGPT serves as a code executing agent that can generate, execute, and self-correct its own code. Similar functions can be constructed for \acrshort{acr:sql}, making it possible for \glspl{acr:llm} to work against relational databases. Furthermore, as \cite{eletiFunctionCallingOther2023} describes, function calling can also be used to extract structured data from text.

An initiative amongst researchers at Berkeley \citep{yanfanjiaBerkeleyFunctionCalling2024} lead to the creation of a benchmark that aims to evaluate the \acrshort{acr:llm}'s ability to call functions and tools. The benchmark, named \gls{acr:bfcl}, includes four different test scenarios: \textit{single function}, where the \acrshort{acr:llm} is provided with a single function definition; \textit{multiple function}, where 2 to 4 functions are passed, and the model must select the appropriate function; \textit{parallel function}, where the model needs to determine how many functions should be called; and \textit{parallel multiple function}, a combination of parallel function and multiple function. Some models support different levels of function calling natively, while others have to be carefully prompted to help accommodate function calling abilities. As of \today, a prompted version of \acrshort{acr:gpt}-4-0125-Preview tops the leaderboard.\footnote{\url{https://gorilla.cs.berkeley.edu/leaderboard.html} (last visited on \today)}

\section{LangChain}
\label{sec:langchain}

\textit{LangChain} \citep{langchainaiLangchainaiLangchain2022} is an open-source project that provides tooling which simplifies the way that developers interface with \glspl{acr:llm}. As the name suggests, LangChain revolves around \textit{chains}. Chains are \glspl{acr:dag}, which can be constructed from so-called \textit{runnables}. LangChain is supported by a large community of developers, and has components that simplify interaction with \acrshortpl{acr:llm}, like prompt templates, output parsers, toolkits for function calling purposes, and off-the-shelf agents.

Common use cases for LangChain are:

\begin{itemize}
    \item Building chatbots for question answering that use semantic retrieval from a document store.
    \item Creating agents with access to external tools by leveraging function calling.
    \item Creating code executing agents for Python, \acrshort{acr:sql}, or other programming languages.
\end{itemize}

In January 2024, LangChain AI rolled out a framework called LangGraph, which is built on top of the LangChain ecosystem. While \textit{chains} are well-suited for \gls{acr:dag} workflows, they are not easily adapted to cyclic graphs. LangGraph, however, is specifically designed to simplify development of cyclic graphs for \acrshort{acr:llm} applications, which are commonly used to create agent-like behaviours \citep{langchainaiLangchainaiLanggraph2024}. A LangGraph \textit{graph} is a set of nodes that pass around and modify a state dictionary. The nodes are connected by edges that specify which node can follow another. Edges can be conditional, meaning the state produced by the originating node decides which edge is followed to the next node. This allows for complex logic and simplifies implementation of advanced agent patterns, some of which are discussed in \autoref{subsec:agent-patterns}.

% \newpage

\section{Geospatial Databases and Data Catalogues}
\label{sec:geo-dbs-and-data-catalogues}

This section will discuss the geospatial technologies that were used in this master's thesis.

\subsection{PostGIS}
\label{subsec:postgis}

\textit{PostGIS} \citep{PostGIS2001} is an open-source extension for the PostgreSQL database that adds support for storing, indexing, and querying geospatial data. Data can be stored in both two and three dimensions, and they can take the form of points, lines, polygons, and more. Geospatial types can be stored along with a spatial index, which can significantly reduce search time for their geometries. GiST (Generalized Search Tree)\footnote{\url{https://en.wikipedia.org/wiki/GiST} (last visited on 2nd June 2024)} is commonly used in PostGIS to take advantage of various tree-based search algorithms that are developed to retrieve spatial features quickly.

PostGIS also comes with a plethora of spatial database functions that are used to analyse and process geospatial data. These function's names are prefixed with \texttt{ST\_} (short for \enquote{spatio-temporal}), with examples including \texttt{ST\_DWITHIN}, \texttt{ST\_BUFFER}, and \texttt{ST\_TRANSFORM}. \autoref{code:postgis-example} shows a typical PostGIS query for retrieving building outlines within a bounding box.

\begin{lstlisting}[
    language=SQL,
    caption={[PostGIS example code]PostGIS example code for retrieving outlines of buildings of type 'house' within a bounding box specified by coordinates in the WGS 84 map projection},
    label=code:postgis-example
]
SELECT * 
FROM osm_buildings_polygons 
WHERE type = 'house' 
  AND ST_Intersects(geom, ST_MakeEnvelope(min_lon, min_lat, max_lon, max_lat, 4326));
\end{lstlisting}

\subsection[OGC API Features]{\acrshort{acr:ogc} \acrshort{acr:api} Features}
\label{subsec:ogc-api-features}

\textit{\acrshort{acr:ogc} \acrshort{acr:api} Features} is an \acrshort{acr:api} specification developed by the \acrfull{acr:ogc} that defines modular \acrshort{acr:api} building blocks for interacting with \textit{features}, which are real-world objects \citep{opengeospatialconsortiumOGCAPIFeatures2022}. This includes blocks for creating, modifying, and querying features on the Web. A typical \acrshort{acr:ogc} \acrshort{acr:api} Features server implements these building blocks for \acrshort{acr:html}, GeoJSON, and \acrshort{acr:gml}. These are called \textit{requirement classes}, though none of them are strictly required. The \acrshort{acr:html} requirement class gives the consumer of the \acrshort{acr:api} a visualization of the features in a web map in the browser, whereas the GeoJSON and \acrshort{acr:gml} requirements classes are typically meant for use in other applications. An \acrshort{acr:ogc} \acrshort{acr:api} Features server consists of a number of collections containing items/features. \autoref{fig:oaf-collections-items-features} illustrates the structure of such a server.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ogc_api_features.png}
    \caption[The general structure of a typical OGC API Features server]{The general structure of a typical \acrshort{acr:ogc} \acrshort{acr:api} Features server. The server would have a number of collections, which in turn have a number of items/features that have geographical components. The consumer of the \acrshort{acr:api} can request a collection of items, or individual features. The figure was retrieved from \url{https://features.developer.ogc.org/} on 29th April 2024.}
    \label{fig:oaf-collections-items-features}
\end{figure}

The development of the \acrshort{acr:ogc} \acrshort{acr:api} Features specification is divided into several parts that are meant to build on each other. Below are the five parts that are listed on \acrshort{acr:ogc}'s websites:\footnote{\url{https://ogcapi.ogc.org/features/} (last visited on 2nd June 2024)}

\begin{itemize}
    \item Features - Part 1: Core\footnote{\url{https://docs.opengeospatial.org/is/17-069r4/17-069r4.html} (last visited on 2nd June 2024)}
    \item Features - Part 2: Coordinate Reference Systems by Reference \footnote{\url{https://docs.ogc.org/is/18-058r1/18-058r1.html} (last visited on 2nd June 2024)}
    \item Features - Part 3: Filtering\footnote{\url{https://docs.ogc.org/DRAFTS/19-079r1.html} (last visited on 2nd June 2024)}
    \item Features - Part 4: Create, Replace, Update and Delete\footnote{\url{https://docs.ogc.org/DRAFTS/20-002.html} (last visited on 2nd June 2024)}
    \item Features - Part 5: Schemas\footnote{\url{https://docs.ogc.org/DRAFTS/23-058r1.html} (last visited on 2nd June 2024)}
\end{itemize}

Part 1 specifies the core capabilities which were described in the first paragraph of this section, while parts 2-5 specify additional capabilities. Part 2 allows for retrieval of features in \glspl{acr:crs} different to the default WGS 84 reference system. Part 3 enables filtering of features using \gls{acr:cql}, which is a language similar to \acrshort{acr:sql}, so that consumers of the \acrshort{acr:api} can retrieve subsets of a given collection. \autoref{code:cql-examples} shows two examples of \gls{acr:cql} queries:

\begin{lstlisting}[
    language=SQL,
    caption={[Examples of CQL filters]Examples of two simple \acrshort{acr:cql} filters: Example 1 filters for all features whose \texttt{county} attribute is either 'Akershus', 'Buskerud', or 'Ostfold'; Example 2 checks whether a feature's geometry is within 1 kilometer of the point specified by the latitude and longitude 63.4265, 10.3960.},
    label=code:cql-examples,
    float, floatplacement=H
]
\\ Example 1
county in ('Akershus', 'Buskerud', 'Ostfold')

\\ Example 2
DWITHIN(the\_geom, Point(63.4265, 10.3960), 1, kilometers)
\end{lstlisting}

Part 4 defines how an \acrshort{acr:ogc} \acrshort{acr:api} Features server should handle addition, replacement, modification, and removal of features. Part 5 explains how features can be defined by a logical schema and how these schemas are published. In addition to parts 1 through 5, several other \textit{proposed} extensions have emerged, such as the \textit{Search} extension, which would allow for multi-collection queries, or the \textit{Geometry Simplification} extension which proposes the use of simplification algorithms for retrieving simplified versions of a collection.\footnote{\url{https://github.com/opengeospatial/ogcapi-features/tree/master/proposals} (last visited on 2nd June 2024)}


% \subsection[SpatioTemporal Asset Catalogs]{\acrlong{acr:stac}}
% \label{subsec:stac}

% The \gls{acr:stac} specification\footnote{\url{https://stacspec.org/en}} is closely related to \acrshort{acr:ogc} \acrshort{acr:api} Features, and Chris Holmes, former board member of \acrlong{acr:ogc}, stated in a blog post that \enquote{\acrshort{acr:stac} \acrshort{acr:api} implements and extends the \acrshort{acr:ogc} \acrshort{acr:api} — Features standard, and our shared goal is for \acrshort{acr:stac} \acrshort{acr:api} to become a full \acrshort{acr:ogc} standard} \citep{holmesSpatioTemporalAssetCatalogs2021a}. The main difference between \acrshort{acr:ogc} \acrshort{acr:api} Features and \acrshort{acr:stac} is the latter's requirement that all items/features should have a temporal component, thus making it \textit{spatio-temporal}.


\section{Related Work}
\label{sec:related-work}

The related work is divided into two main parts: \autoref{subsec:llm-gis}, which presents research investigating potential use cases of \acrshortpl{acr:llm} in the field of geospatial information technology; and \autoref{subsec:agent-patterns}, which includes examples of three different types of patterns that can be used in \acrshort{acr:llm}-based agents.

\subsection[Using LLMs for Geospatial Purposes]{Using \acrshortpl{acr:llm} for Geospatial Purposes}
\label{subsec:llm-gis}

\cite{robertsGPT4GEOHowLanguage2023} investigated the extent of GPT-4's geospatial awareness through a set of case studies with increasing difficulty, starting with general factual tasks and moving up to more complex questions such as generating country outlines and travel networks. They found that \acrshort{acr:gpt}-4 is \enquote{skillful at solving a variety of application-centric tasks}, almost having the ability to \enquote{see}, despite the fact that they only have the ability to read and generate \textit{text}.\footnote{Note that multi-modal models were not particularly widespread when they wrote their paper.} \citeauthor{robertsGPT4GEOHowLanguage2023} showed that the \acrshort{acr:llm} can function as a travel assistant, providing trip suggestions when given requirements for the trip, and that it can give more or less correct start- and end locations of certain bird migration paths. While it quickly became obvious that a lot of geospatial context has been embedded within the model during its vast pre-training, the open question of whether this is memorization or reasoning is a central one. The variability of tasks the experiments leads the authors to conclude that it is unlikely that all of this is memorization, but they say that some things appear to be memorized.

\cite{mooneyUnderstandingGeospatialSkills2023} examined the performance of ChatGPT in a \acrshort{acr:gis} exam, aiming to evaluate its understanding of various geospatial concepts, highlighting the model's capabilities and limitations. Both GPT-3.5 and GPT-4 were tested, and they delivered performances equivalent to grades of D and B+, respectively. Additional experiments were conducted for more specialized areas of \acrshort{acr:gis}, including True/False questions about spatial analysis, and simple tasks in applied \acrshort{acr:gis} workflows. Experiments on the latter showed that GPT-4 was able to correctly answer a relatively complex \acrshort{acr:gis} task involving seven different datasets, which required seven correct steps in order to obtain a perfect score. Generally, GPT-4 outperformed GPT-3 across all tasks. Both models clearly showcased their strong capabilities, but the authors highlight a range of challenges if such models are to be applied for real tasks. For instance, they write that the multi-modal nature of \acrshort{acr:gis} makes it difficult to apply the \acrshortpl{acr:llm} directly.

\cite{liAutonomousGISNextgeneration2023} state that \enquote{autonomous \acrshort{acr:gis} will need to achieve five autonomous goals: self-generating, self-organizing, self-verifying, self-executing, and self-growing}. They propose a divide-and-conquer-based method to address some of these goals, where they split the overall problem into sub-problems. Furthermore, they propose a simple trial-and-error approach to address the self-verifying goal. Even with its shortcomings, the solution that \citeauthor{liAutonomousGISNextgeneration2023} provide, called \textit{\acrshort{acr:llm}-Geo}, produced good results in various case studies by providing executable assemblies in a Python environment when given URLs to relevant datasets along with a user-specified geospatial query.

\cite{zhangGeoGPTUnderstandingProcessing2023} use the LangChain framework to combine different GIS tools in a sequence to solve various sub-goals, focusing on using the semantic understanding and reasoning abilities of \acrshortpl{acr:llm} to call externally defined tools. They named this framework \textit{GeoGPT}.\footnote{This name was borrowed for the proposed solution to an \acrshort{acr:llm}-based \acrshort{acr:gis} in this master's thesis, which is presented in \autoref{cha:architecture}.} The externally defined tools are described by names and descriptions, the latter of which contains information about the function's input parameters and output types. Tools are defined for the collection, processing, analysis, and visualization of geospatial data. The effectiveness of the system is showcased through case studies, where GeoGPT performed actions like land use classification and raster-to-vector conversion using the externally defined tools.


\subsection{Agent Patterns}
\label{subsec:agent-patterns}

\acrshort{acr:llm}-based agents can be implemented in many different ways, and researchers have developed a plethora of \textit{agent patterns} that seek to make improvements to areas where \acrshortpl{acr:llm} tend to be less effective. This section will briefly explain three different types of agent patterns that were considered during the development of GeoGPT.

The \textbf{multi-agent} pattern takes inspiration from human collaboration, consisting of multiple specialized agents that work together to achieve a common objective. MetaGPT \citep{hongMetaGPTMetaProgramming2023} is an \acrshort{acr:llm}-based multi-agent system consisting of agents with human-level domain expertise. Using an assembly line paradigm, where the overall goal is divided into subtasks, \citeauthor{hongMetaGPTMetaProgramming2023} showed that MetaGPT could generate more coherent solutions compared to the previous state-of-the-art multi-agent systems. MetaGPT achieved a new state-of-the-art performance on the HumanEval \citep{chanChatEvalBetterLLMbased2023} and \gls{acr:mbpp} \citep{austinProgramSynthesisLarge2021} benchmarks, demonstrating the potential of the multi-agent pattern \citep[7]{hongMetaGPTMetaProgramming2023}. AutoGen \citep{wuAutoGenEnablingNextGen2023a} and crewAI \citep{mouraJoaomdmouraCrewAI2024} are other examples of frameworks geared towards the multi-agent pattern.

Patterns that employ \textbf{self-reflection} are commonly used with autonomous \acrshort{acr:llm}-based agents. \textit{Reflexion} \citep{shinnReflexionLanguageAgents2023} is a framework that reinforces agents through linguistic feedback, essentially allowing the agent to reflect upon the outcomes of its actions. The framework utilizes three distinct models: an \textit{Actor} model responsible for generating text and actions; an \textit{Evaluator} model which assesses the quality of the outputs from the Actor; and a \textit{Self-Reflection} model that generates reinforcement cues for the Actor based on the output and quality assessment of the other two models. Together, these three models form a loop that will run until the Evaluator deems the output from the Actor to be correct.

\textbf{Step-by-step reasoning} is another pattern that has proved to be efficient in helping \acrshortpl{acr:llm} produce correct responses. \cite{weiChainofThoughtPromptingElicits2023a} demonstrated that so-called \textit{chain-of-thought prompting} can be used for enhancing reasoning in \acrshortpl{acr:llm}, where the prompt to the \acrshort{acr:llm} includes examples of how to reason for tasks \textit{similar} to the one being solved. By helping the \acrshort{acr:llm} with decomposing multi-step problems into intermediate steps, \citeauthor{weiChainofThoughtPromptingElicits2023a} managed to achieve state-of-the-art accuracy on the GSM8K benchmark of maths word problems \citep{cobbeTrainingVerifiersSolve2021}.


\glsresetall
